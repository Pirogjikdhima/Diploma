{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torchcrf import CRF\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functions.file import *\n",
    "\n",
    "lines = get_lines_from_ner_corpus(\"../../Corpus/korpusi.txt\")\n",
    "write_lines_to_csv(lines, \"korpusi.csv\")"
   ],
   "id": "2cd3839872240f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"korpusi.csv\")"
   ],
   "id": "2d4bb9936f653a9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Total number of sentences in the dataset: {:,}\".format(data_df[\"Sentence #\"].nunique()))\n",
    "print(\"Total tokens in the dataset: {:,}\".format(data_df.shape[0]))"
   ],
   "id": "ea93efb8576b3478",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ner_counts = data_df[\"NER_Tag\"].value_counts()\n",
    "ner_counts"
   ],
   "id": "f25141dfa6432a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "word_counts = data_df.groupby(\"Sentence #\")[\"Word\"].agg([\"count\"])\n",
    "word_counts = word_counts.rename(columns={\"count\": \"Word count\"})"
   ],
   "id": "aa529f7c8e119184",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# all_words = list(set(data_df[\"Word\"].values))\n",
    "# all_tags = list(set(data_df[\"NER_Tag\"].values))\n",
    "#\n",
    "# print(\"Number of unique words: {}\".format(data_df[\"Word\"].nunique()))\n",
    "# print(\"Number of unique tags : {}\".format(data_df[\"NER_Tag\"].nunique()))\n",
    "unique_words = sorted(list(set(data_df[\"Word\"].str.lower().values)))\n",
    "unique_tags = sorted(list(set(data_df[\"NER_Tag\"].values)))\n",
    "\n",
    "print(f\"Unique words: {len(unique_words)}\")\n",
    "print(f\"Unique tags: {len(unique_tags)}\")\n",
    "print(f\"Tags: {unique_tags}\")"
   ],
   "id": "ff3c44938bd104e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "word2index = {word:idx + 2 for idx, word in enumerate(all_words) }\n",
    "word2index[\"<UNK>\"]=0\n",
    "word2index[\"<PAD>\"]=1\n",
    "word2index = dict([(k, v) for k, v in sorted(word2index.items(), key=lambda item: item[1])])\n",
    "\n",
    "index2word = {idx:word for word,idx in word2index.items()}"
   ],
   "id": "ff885cd43108db1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "tag2index[\"<PAD>\"] = 0\n",
    "tag2index = dict([(k, v) for k, v in sorted(tag2index.items(), key=lambda item: item[1])])\n",
    "\n",
    "index2tag = {idx: word for word, idx in tag2index.items()}"
   ],
   "id": "b64f2bc992400ccc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"Word\"].values.tolist(),\n",
    "                   data[\"NER_Tag\"].values.tolist()\n",
    "                   )\n",
    "    return [(word, tag) for word, tag in iterator]\n",
    "\n",
    "sentences = data_df.groupby(\"Sentence #\").apply(to_tuples, include_groups=False).tolist()\n",
    "\n",
    "print(sentences[0])"
   ],
   "id": "d5fbcf12a29524db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = [[word[0] for word in sentence] for sentence in sentences]\n",
    "Y = [[word[1] for word in sentence] for sentence in sentences]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"Y[0]:\", Y[0])\n",
    "\n",
    "X = [[word2index[word] for word in sentence] for sentence in X]\n",
    "Y = [[tag2index[tag] for tag in sentence] for sentence in Y]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"Y[0]:\", Y[0])"
   ],
   "id": "b08e24ca70db89e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MAX_SENTENCE = word_counts.max().iloc[0]\n",
    "print(\"Longest sentence in the corpus contains {} words.\".format(MAX_SENTENCE))"
   ],
   "id": "a51861fe163ed46a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = [sentence + [word2index[\"<PAD>\"]] * (MAX_SENTENCE - len(sentence)) for sentence in X]\n",
    "Y = [sentence + [tag2index[\"<PAD>\"]] * (MAX_SENTENCE - len(sentence)) for sentence in Y]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"Y[0]:\", Y[0])"
   ],
   "id": "11d3efeba37f3bc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T17:47:37.318790Z",
     "start_time": "2025-09-04T17:47:37.266442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Group by Sentence ID and calculate the length of each sentence\n",
    "sentence_lengths = data_df.groupby(\"Sentence #\")[\"Word\"].count().reset_index()\n",
    "sentence_lengths.columns = [\"Sentence ID\", \"Length\"]\n"
   ],
   "id": "1a2cddafd8bc7d15",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TAG_COUNT = len(tag2index)\n",
    "\n",
    "y = np.array(Y)\n",
    "Y = [np.eye(TAG_COUNT)[sentence] for sentence in Y]\n",
    "print(\"X[0]:\", X[0])\n",
    "print(\"y[0]:\", y[0])\n",
    "print(\"Y[0]:\", Y[0])"
   ],
   "id": "24e3048d2ac5b17f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=1234)\n",
    "\n",
    "print(\"Number of sentences in the training dataset: {}\".format(len(X_train)))\n",
    "print(\"Number of sentences in the test dataset : {}\".format(len(X_test)))"
   ],
   "id": "741c97e87f5e852d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ],
   "id": "e396f19ffcbd273a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "a3396fd79cab73f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float)  # one-hot labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NERDataset(X_train, y_train)\n",
    "test_dataset  = NERDataset(X_test, y_test)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ],
   "id": "6ef41778e790a6a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BiLSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_count, embedding_dim=100, hidden_dim=64):\n",
    "        super(BiLSTM_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2index[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim*2, tag_count)  # bidirectional -> *2\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "id": "979ee1bd4f7321f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VOCAB_SIZE = len(word2index)\n",
    "TAG_COUNT  = len(tag2index)\n",
    "\n",
    "model = BiLSTM_NER(VOCAB_SIZE, TAG_COUNT, embedding_dim=200, hidden_dim=64).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag2index[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "76f16eb6beed9bee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        y_batch_indices = torch.argmax(y_batch, dim=2)\n",
    "\n",
    "        outputs = outputs.view(-1, TAG_COUNT)\n",
    "        y_batch_indices = y_batch_indices.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, y_batch_indices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ],
   "id": "64c6af35339f9ff4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:40.856262Z",
     "start_time": "2025-09-04T16:54:37.361035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functions.file import *"
   ],
   "id": "9c5eb7b5283f4372",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:45.808002Z",
     "start_time": "2025-09-04T16:54:44.989800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lines = get_lines_from_ner_corpus(\"../../Corpus/korpusi.txt\")\n",
    "write_lines_to_csv(lines, \"korpusi.csv\")"
   ],
   "id": "b9a805cd3852a86f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:47.376934Z",
     "start_time": "2025-09-04T16:54:47.127074Z"
    }
   },
   "cell_type": "code",
   "source": "data_df = pd.read_csv(\"korpusi.csv\")",
   "id": "e7b0ea353200f57c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:48.608329Z",
     "start_time": "2025-09-04T16:54:48.604300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Dataset shape:\", data_df.shape)\n",
    "print(\"Sample data:\")\n",
    "print(data_df.head())"
   ],
   "id": "55f877e44a84a828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1003903, 3)\n",
      "Sample data:\n",
      "   Sentence #   Word NER_Tag\n",
      "0           1  Bujar   B-PER\n",
      "1           1  Gjoka   I-PER\n",
      "2           1      :       O\n",
      "3           1  Gjuha       O\n",
      "4           1  është       O\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:51.487800Z",
     "start_time": "2025-09-04T16:54:51.416103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_df = data_df.dropna()  # Remove any NaN values\n",
    "print(f\"After cleaning: {len(data_df)} rows\")"
   ],
   "id": "d07af9765094534c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 1003903 rows\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:52.313752Z",
     "start_time": "2025-09-04T16:54:52.280395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze class distribution\n",
    "ner_counts = data_df[\"NER_Tag\"].value_counts()\n",
    "print(\"\\nNER Tag Distribution:\")\n",
    "print(ner_counts)"
   ],
   "id": "d3f37da938421935",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Tag Distribution:\n",
      "NER_Tag\n",
      "O           914052\n",
      "B-PER        20190\n",
      "I-DATE_0     13790\n",
      "I-PER        12599\n",
      "B-VEND_0     11689\n",
      "B-VEND_1      8764\n",
      "I-ORG         6459\n",
      "B-DATE_0      6135\n",
      "B-ORG         4526\n",
      "I-VEND_1      1615\n",
      "I-EVENT       1038\n",
      "B-EVENT        648\n",
      "I-DATE_1       470\n",
      "B-DATE_1       459\n",
      "B-PRO          343\n",
      "I-VEND_0       323\n",
      "I-RRUGE        279\n",
      "I-SHESH        171\n",
      "B-RRUGE        140\n",
      "B-SHESH        112\n",
      "I-PRO          101\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:54.466091Z",
     "start_time": "2025-09-04T16:54:54.268864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_words = sorted(list(set(data_df[\"Word\"].str.lower().values)))\n",
    "unique_tags = sorted(list(set(data_df[\"NER_Tag\"].values)))\n",
    "\n",
    "print(f\"Unique words: {len(unique_words)}\")\n",
    "print(f\"Unique tags: {len(unique_tags)}\")\n",
    "print(f\"Tags: {unique_tags}\")"
   ],
   "id": "d8614ce382969137",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 55578\n",
      "Unique tags: 21\n",
      "Tags: ['B-DATE_0', 'B-DATE_1', 'B-EVENT', 'B-ORG', 'B-PER', 'B-PRO', 'B-RRUGE', 'B-SHESH', 'B-VEND_0', 'B-VEND_1', 'I-DATE_0', 'I-DATE_1', 'I-EVENT', 'I-ORG', 'I-PER', 'I-PRO', 'I-RRUGE', 'I-SHESH', 'I-VEND_0', 'I-VEND_1', 'O']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:56.117482Z",
     "start_time": "2025-09-04T16:54:56.096351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, word in enumerate(unique_words):\n",
    "    word2idx[word] = i + 2"
   ],
   "id": "c055404e5287964b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:54:57.776047Z",
     "start_time": "2025-09-04T16:54:57.771332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tag2idx = {\"<PAD>\": 0}\n",
    "for i, tag in enumerate(unique_tags):\n",
    "    tag2idx[tag] = i + 1"
   ],
   "id": "400ac86051c8b411",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:00.023546Z",
     "start_time": "2025-09-04T16:55:00.010747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "print(f\"Final vocab sizes - Words: {len(word2idx)}, Tags: {len(tag2idx)}\")"
   ],
   "id": "917f6cf0428e2161",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocab sizes - Words: 55580, Tags: 22\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:01.766749Z",
     "start_time": "2025-09-04T16:55:01.763801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to sentences\n",
    "def to_tuples(group):\n",
    "    return list(zip(group[\"Word\"].values, group[\"NER_Tag\"].values))"
   ],
   "id": "44af7b2e9b625328",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:04.554448Z",
     "start_time": "2025-09-04T16:55:03.393482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = data_df.groupby(\"Sentence #\").apply(to_tuples,include_groups=False).tolist()\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Sample sentence: {sentences[0][:5]}...\")"
   ],
   "id": "9ba3a5e09eca006b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 37785\n",
      "Sample sentence: [('Bujar', 'B-PER'), ('Gjoka', 'I-PER'), (':', 'O'), ('Gjuha', 'O'), ('është', 'O')]...\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:05.709016Z",
     "start_time": "2025-09-04T16:55:05.517863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to sequences\n",
    "X_words = [[word for word, tag in sentence] for sentence in sentences]\n",
    "y_tags = [[tag for word, tag in sentence] for sentence in sentences]"
   ],
   "id": "1fab6e5309b5ae34",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:07.828827Z",
     "start_time": "2025-09-04T16:55:07.824908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to indices with careful bounds checking\n",
    "def words_to_indices(word_sequences):\n",
    "    result = []\n",
    "    for sequence in word_sequences:\n",
    "        indices = []\n",
    "        for word in sequence:\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in word2idx:\n",
    "                indices.append(word2idx[word_lower])\n",
    "            else:\n",
    "                indices.append(word2idx[\"<UNK>\"])\n",
    "        result.append(indices)\n",
    "    return result\n",
    "\n",
    "def tags_to_indices(tag_sequences):\n",
    "    result = []\n",
    "    for sequence in tag_sequences:\n",
    "        indices = []\n",
    "        for tag in sequence:\n",
    "            if tag in tag2idx:\n",
    "                indices.append(tag2idx[tag])\n",
    "            else:\n",
    "                print(f\"Warning: Unknown tag '{tag}', using <PAD>\")\n",
    "                indices.append(tag2idx[\"<PAD>\"])\n",
    "        result.append(indices)\n",
    "    return result"
   ],
   "id": "b6639c028a2d5463",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:52.264207Z",
     "start_time": "2025-09-04T16:55:52.014982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_indices = words_to_indices(X_words)\n",
    "y_indices = tags_to_indices(y_tags)"
   ],
   "id": "47556da1b878c51d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:53.836332Z",
     "start_time": "2025-09-04T16:55:53.806730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify indices are in correct range\n",
    "max_word_idx = max([max(seq) if seq else 0 for seq in X_indices])\n",
    "max_tag_idx = max([max(seq) if seq else 0 for seq in y_indices])"
   ],
   "id": "31dc459627ed98c0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:55.805031Z",
     "start_time": "2025-09-04T16:55:55.801700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Max word index: {max_word_idx} (vocab size: {len(word2idx)})\")\n",
    "print(f\"Max tag index: {max_tag_idx} (tag count: {len(tag2idx)})\")\n",
    "\n",
    "assert max_word_idx < len(word2idx), f\"Word index out of range: {max_word_idx} >= {len(word2idx)}\"\n",
    "assert max_tag_idx < len(tag2idx), f\"Tag index out of range: {max_tag_idx} >= {len(tag2idx)}\""
   ],
   "id": "54521e31cf3b8271",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word index: 55579 (vocab size: 55580)\n",
      "Max tag index: 21 (tag count: 22)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:55:58.892012Z",
     "start_time": "2025-09-04T16:55:58.884255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate sequence lengths\n",
    "seq_lengths = [len(seq) for seq in X_indices]\n",
    "max_len = max(seq_lengths)\n",
    "avg_len = np.mean(seq_lengths)\n",
    "p95_len = int(np.percentile(seq_lengths, 95))\n",
    "\n",
    "print(f\"Sequence lengths - Max: {max_len}, Avg: {avg_len:.1f}, 95th percentile: {p95_len}\")"
   ],
   "id": "883951dc2bc49704",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths - Max: 333, Avg: 26.6, 95th percentile: 56\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:00.722600Z",
     "start_time": "2025-09-04T16:56:00.719239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use 95th percentile to reduce padding\n",
    "# MAX_LEN = p95_len\n",
    "\n",
    "# Use absolute max length\n",
    "MAX_LEN = max_len"
   ],
   "id": "7992224171b86ffd",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:02.008753Z",
     "start_time": "2025-09-04T16:56:02.005850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pad sequences\n",
    "def pad_sequences(sequences, max_len, pad_value):\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= max_len:\n",
    "            padded.append(seq[:max_len])  # Truncate\n",
    "        else:\n",
    "            padded.append(seq + [pad_value] * (max_len - len(seq)))  # Pad\n",
    "    return padded"
   ],
   "id": "93ad642f541cba46",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:03.644804Z",
     "start_time": "2025-09-04T16:56:03.464378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_padded = pad_sequences(X_indices, MAX_LEN, word2idx[\"<PAD>\"])\n",
    "y_padded = pad_sequences(y_indices, MAX_LEN, tag2idx[\"<PAD>\"])"
   ],
   "id": "78d4f4b37bdefeb4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:06.559780Z",
     "start_time": "2025-09-04T16:56:06.552872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify padded sequences\n",
    "print(f\"Padded sequence length: {len(X_padded[0])}\")\n",
    "print(f\"All X sequences same length: {len(set(len(seq) for seq in X_padded)) == 1}\")\n",
    "print(f\"All y sequences same length: {len(set(len(seq) for seq in y_padded)) == 1}\")"
   ],
   "id": "2e38c1e5ff5d2231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequence length: 333\n",
      "All X sequences same length: True\n",
      "All y sequences same length: True\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:08.077206Z",
     "start_time": "2025-09-04T16:56:08.052286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, y_padded, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42\n",
    ")"
   ],
   "id": "8d27d2f9bfea9f7c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:09.492597Z",
     "start_time": "2025-09-04T16:56:09.489645Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Data splits - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")",
   "id": "c20265039367e86b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits - Train: 27205, Val: 3023, Test: 7557\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:11.220195Z",
     "start_time": "2025-09-04T16:56:11.217249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "a74b3522e1662938",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:13.283171Z",
     "start_time": "2025-09-04T16:56:12.370464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_t = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.long).to(device)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)"
   ],
   "id": "204eb3a5113594d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:14.494024Z",
     "start_time": "2025-09-04T16:56:14.490837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "51b738fcd83c2f02",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:15.683289Z",
     "start_time": "2025-09-04T16:56:15.679305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model definition\n",
    "class BiLSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_count, embedding_dim=128, hidden_dim=64, dropout=0.3,num_layers=1):\n",
    "        super(BiLSTM_NER, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2idx[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, tag_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch, seq_len, hidden_dim*2)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = self.classifier(lstm_out)  # (batch, seq_len, tag_count)\n",
    "        return output\n"
   ],
   "id": "29ff1cf028e1185a",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:17.953576Z",
     "start_time": "2025-09-04T16:56:17.950826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create datasets and loaders\n",
    "train_dataset = NERDataset(X_train_t, y_train_t)\n",
    "val_dataset = NERDataset(X_val_t, y_val_t)\n",
    "test_dataset = NERDataset(X_test_t, y_test_t)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "id": "d7ba60a28bc21938",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:19.190592Z",
     "start_time": "2025-09-04T16:56:19.187124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = len(word2idx)\n",
    "tag_count = len(tag2idx)"
   ],
   "id": "700b47598cba26f6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:33.426597Z",
     "start_time": "2025-09-04T16:56:33.374631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "model = BiLSTM_NER(vocab_size, tag_count, embedding_dim=200, hidden_dim=128,num_layers=1,dropout=0).to(device)"
   ],
   "id": "4cc77ed75e1b57c",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:50.011796Z",
     "start_time": "2025-09-04T16:56:48.382076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "e935f428766f11bd",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:51.267829Z",
     "start_time": "2025-09-04T16:56:51.263242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Model initialized successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "82b3863bc0673ca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n",
      "Model parameters: 11,459,574\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:56:53.342381Z",
     "start_time": "2025-09-04T16:56:53.243810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if any index in X_batch is out of range\n",
    "invalid_indices = (X_batch >= vocab_size).nonzero(as_tuple=True)\n",
    "if len(invalid_indices[0]) > 0:\n",
    "    print(f\"Invalid indices found: {X_batch[invalid_indices]}\")\n",
    "    print(f\"Max index in X_batch: {X_batch.max()}, Vocab size: {vocab_size}\")"
   ],
   "id": "23b0e031bb31a788",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Check if any index in X_batch is out of range\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m invalid_indices = (\u001B[43mX_batch\u001B[49m >= vocab_size).nonzero(as_tuple=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(invalid_indices[\u001B[32m0\u001B[39m]) > \u001B[32m0\u001B[39m:\n\u001B[32m      4\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid indices found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX_batch[invalid_indices]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'X_batch' is not defined"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:57:49.393724Z",
     "start_time": "2025-09-04T16:56:59.189591Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Training loop\n",
    "epochs = 3\n",
    "best_val_f1 = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X_batch)  # (batch, seq_len, tag_count)\n",
    "        outputs = outputs.view(-1, tag_count)  # (batch*seq_len, tag_count)\n",
    "        targets = y_batch.view(-1)  # (batch*seq_len,)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            outputs_flat = outputs.view(-1, tag_count)\n",
    "            targets_flat = y_batch.view(-1)\n",
    "\n",
    "            loss = criterion(outputs_flat, targets_flat)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            y_true.extend(y_batch.cpu().numpy().flatten())\n",
    "            y_pred.extend(predictions.cpu().numpy().flatten())\n",
    "\n",
    "    # Calculate metrics (excluding padding)\n",
    "    mask = np.array(y_true) != tag2idx[\"<PAD>\"]\n",
    "    y_true_clean = np.array(y_true)[mask]\n",
    "    y_pred_clean = np.array(y_pred)[mask]\n",
    "\n",
    "    val_acc = accuracy_score(y_true_clean, y_pred_clean)\n",
    "    val_f1 = f1_score(y_true_clean, y_pred_clean, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    print(f\"  Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(\"-\" * 40)"
   ],
   "id": "4b39cca64efa252",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     15\u001B[39m targets = y_batch.view(-\u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# (batch*seq_len,)\u001B[39;00m\n\u001B[32m     17\u001B[39m loss = criterion(outputs, targets)\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Gradient clipping\u001B[39;00m\n\u001B[32m     21\u001B[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001B[32m1.0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Personal\\Diploma\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Personal\\Diploma\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Personal\\Diploma\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load best model and test\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "y_true_test, y_pred_test = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        predictions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "        y_true_test.extend(y_batch.cpu().numpy().flatten())\n",
    "        y_pred_test.extend(predictions.cpu().numpy().flatten())"
   ],
   "id": "d2e3d6706f0b3f4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final test metrics\n",
    "mask = np.array(y_true_test) != tag2idx[\"<PAD>\"]\n",
    "y_true_final = np.array(y_true_test)[mask]\n",
    "y_pred_final = np.array(y_pred_test)[mask]\n",
    "\n",
    "test_acc = accuracy_score(y_true_final, y_pred_final)\n",
    "test_f1 = f1_score(y_true_final, y_pred_final, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL TEST RESULTS:\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classification report\n",
    "tag_names = [idx2tag[i] for i in range(1, len(idx2tag))]  # Exclude <PAD>\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_final, y_pred_final,\n",
    "                          labels=list(range(1, len(tag2idx))),\n",
    "                          target_names=tag_names, zero_division=0))"
   ],
   "id": "23ca3db40ecbebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e69f5b2dc6671c21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e65efd84fba47d88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        predicted = torch.argmax(outputs, dim=2)\n",
    "\n",
    "        y_true.extend(torch.argmax(y_batch, dim=2).cpu().numpy().flatten())\n",
    "        y_pred.extend(predicted.cpu().numpy().flatten())\n",
    "\n",
    "# Compute metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "mask = np.array(y_true) != tag2index[\"<PAD>\"]  # ignore padding\n",
    "y_true_masked = np.array(y_true)[mask]\n",
    "y_pred_masked = np.array(y_pred)[mask]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true_masked, y_pred_masked))\n",
    "print(\"F1 Score:\", f1_score(y_true_masked, y_pred_masked, average='weighted'))\n"
   ],
   "id": "14ec7a0ad576f6d0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
